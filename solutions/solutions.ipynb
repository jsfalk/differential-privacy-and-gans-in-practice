{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and code needed for solutions\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from layers import MultiCategoryGumbelSoftmax\n",
    "from private_db import query_aggregate, query_restricted\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "def show_heatmap(df):\n",
    "    crosstab = pd.crosstab(df.weather, df.status)\n",
    "    releveled = crosstab.loc[['sunny', 'cloudy', 'rainy'], ['on time', 'delayed', 'canceled']]\n",
    "    sns.heatmap(releveled, cmap=\"YlGnBu\")\n",
    "    plt.show()\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {'weather': ['sunny']*10000+['cloudy']*10000+['rainy']*10000,\n",
    "     'status': ['on time']*8000+['delayed']*2000\n",
    "     + ['on time']*3000+['delayed']*5000+['canceled']*2000\n",
    "     + ['on time']*1000+['delayed']*3000+['canceled']*6000}\n",
    ")\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(df)\n",
    "data = torch.tensor(enc.transform(df).toarray()).float()\n",
    "\n",
    "# the multi category Gumbel softmax needs to know the dimensions of each output variable\n",
    "output_dims = [len(cat) for cat in enc.categories_]\n",
    "\n",
    "noise_dim = 8 # number of dimensions for noise input to generator\n",
    "data_dim = data.shape[1] # number of dimensions of the data inputs\n",
    "hidden_dim = 16 # number of dimensions for the hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_heatmap(df):\n",
    "    crosstab = pd.crosstab(df.weather, df.status)\n",
    "    releveled = crosstab.loc[['sunny', 'cloudy', 'rainy'], ['on time', 'delayed', 'canceled']]\n",
    "    sns.heatmap(releveled, cmap=\"YlGnBu\")\n",
    "    plt.savefig('map.png')\n",
    "    \n",
    "show_heatmap(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weather</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sunny</td>\n",
       "      <td>delayed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cloudy</td>\n",
       "      <td>delayed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rainy</td>\n",
       "      <td>on time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sunny</td>\n",
       "      <td>on time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cloudy</td>\n",
       "      <td>delayed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sunny</td>\n",
       "      <td>on time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rainy</td>\n",
       "      <td>on time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rainy</td>\n",
       "      <td>canceled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cloudy</td>\n",
       "      <td>canceled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rainy</td>\n",
       "      <td>delayed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  weather    status\n",
       "0   sunny   delayed\n",
       "1  cloudy   delayed\n",
       "2   rainy   on time\n",
       "3   sunny   on time\n",
       "4  cloudy   delayed\n",
       "5   sunny   on time\n",
       "6   rainy   on time\n",
       "7   rainy  canceled\n",
       "8  cloudy  canceled\n",
       "9   rainy   delayed"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack aggregation (query_aggregate)\n",
    "\n",
    "We can find an individual's income by limiting our average to an average over a single person:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_aggregate('SELECT AVG(income) FROM people WHERE age=99 and zip=60637')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack aggregation (query_restricted)\n",
    "\n",
    "We can use `SUM` or `AVG` to learn an individual's income. `SUM` is easier, but it is helpful to see how `AVG` works since frequently only averages are available. Note that it doesn't really matter exactly what restriction we use after `OR`, as long as it includes at least 50 people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using SUM\n",
    "sum_large = query_restricted('SELECT SUM(income) FROM people WHERE (age=98 AND zip=60616) OR zip=60609')\n",
    "sum_small = query_restricted('SELECT SUM(income) FROM people WHERE zip=60609')\n",
    "sum_large - sum_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using AVG\n",
    "count = query_restricted('SELECT COUNT(income) FROM people WHERE zip=60609') \n",
    "avg_large = query_restricted('SELECT AVG(income) FROM people WHERE (age=98 AND zip=60616) OR zip=60609')\n",
    "avg_small = query_restricted('SELECT AVG(income) FROM people WHERE zip=60609')\n",
    "(count+1)*avg_large - count*avg_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# confirming the successful attack\n",
    "query_aggregate('SELECT AVG(income) FROM people WHERE age=98 AND zip=60616')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian mechanism\n",
    "\n",
    "Note that in order to stop the `SUM` attack we need to add far more noise than to stop the `AVG` attack. It may make more sense to simply prohibit `SUM` queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_private(query_str):\n",
    "    result = query_aggregate(query_str)\n",
    "    noise = np.random.normal(0, 1000)\n",
    "    return result + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's see that `AVG` queries are still relatively accurate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_private('SELECT AVG(income) FROM people WHERE age=30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_restricted('SELECT AVG(income) FROM people WHERE age=30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_private('SELECT AVG(income) FROM people WHERE age=50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_restricted('SELECT AVG(income) FROM people WHERE age=50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try the `AVG` attack (assuming zip code counts are known exactly):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = query_aggregate('SELECT COUNT(*) FROM people WHERE zip=60609') \n",
    "avg_large = query_private('SELECT AVG(income) FROM people WHERE (age=98 AND zip=60616) OR zip=60609')\n",
    "avg_small = query_private('SELECT AVG(income) FROM people WHERE zip=60609')\n",
    "(count+1)*avg_large - (count)*avg_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again note that the `SUM` attack is still largely successful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using SUM\n",
    "sum_large = query_private('SELECT SUM(income) FROM people WHERE (age=98 AND zip=60616) OR zip=60609')\n",
    "sum_small = query_private('SELECT SUM(income) FROM people WHERE zip=60609')\n",
    "sum_large - sum_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more sophisticated approach would tune the noise up or down based on how many entries the `AVG` was operating over:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from private_db import query\n",
    "\n",
    "def query_private_adaptive(query_str):\n",
    "    result, count = query(query_str)\n",
    "    noise = np.random.normal(0, 1e6/count)\n",
    "    return result + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wasserstein GAN\n",
    "\n",
    "We remove the `Sigmoid` layer from the discriminator, and update the training losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generator = torch.nn.Sequential(\n",
    "    torch.nn.Linear(noise_dim, hidden_dim),\n",
    "    torch.nn.ReLU(),\n",
    "    MultiCategoryGumbelSoftmax(hidden_dim, output_dims)\n",
    ")\n",
    "\n",
    "discriminator = torch.nn.Sequential(\n",
    "    torch.nn.Linear(data_dim, hidden_dim),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(hidden_dim, 1)\n",
    "    ### CHANGED: removed Sigmoid ###\n",
    ")\n",
    "\n",
    "def train(data, generator, discriminator,\n",
    "          epochs=20, n_discriminator=5, batch_size=128,\n",
    "          learning_rate=1e-3):\n",
    "    \"\"\"Train the GAN\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : torch.Tensor\n",
    "        Data for training\n",
    "    generator : torch.nn.Sequential\n",
    "        Generator network\n",
    "    discriminator : torch.nn.Sequential\n",
    "        Discriminator network\n",
    "    epochs : int\n",
    "        Number of iterations over the full data set for training\n",
    "    n_discriminator : int\n",
    "        Number of discriminator training iterations\n",
    "    batch_size : int\n",
    "        Number of training examples per inner iteration\n",
    "    learning_rate : float\n",
    "        Learning rate for training\n",
    "    \"\"\"\n",
    "    # these solvers are optimizers for learning the parameters for our networks\n",
    "    # we have one for each network (the generator and the discriminator)\n",
    "    # RMSprop is one of many optimizers, and the choice is not terribly important here\n",
    "    # The learning rate influences how large of a step the optimizer takes when it updates the parameters\n",
    "    generator_solver = torch.optim.RMSprop(\n",
    "        generator.parameters(), lr=learning_rate\n",
    "    )\n",
    "    \n",
    "    discriminator_solver = torch.optim.RMSprop(\n",
    "        discriminator.parameters(), lr=learning_rate\n",
    "    )\n",
    "\n",
    "    # There is a batch for each discriminator training iteration,\n",
    "    # so each epoch is epoch_length iterations, and the total number of\n",
    "    # iterations is the number of epochs times the length of each epoch.\n",
    "    epoch_length = len(data) / (n_discriminator * batch_size)\n",
    "    n_iters = int(epochs * epoch_length)\n",
    "    \n",
    "    # training loop\n",
    "    for iteration in range(n_iters):\n",
    "        for _ in range(n_discriminator):\n",
    "            # Sample real data\n",
    "            rand_perm = torch.randperm(data.size(0))\n",
    "            real_sample = data[rand_perm[:batch_size]]\n",
    "\n",
    "            # Sample fake data\n",
    "            noise = torch.randn(batch_size, noise_dim)\n",
    "            fake_sample = generator(noise)\n",
    "\n",
    "            # Have the discriminator score the data\n",
    "            discriminator_real = discriminator(real_sample)\n",
    "            discriminator_fake = discriminator(fake_sample)\n",
    "\n",
    "            # Calculate discriminator loss\n",
    "            # Discriminator wants to assign a high score to real data\n",
    "            # and a low score to fake data\n",
    "            ### CHANGED: new loss function ###\n",
    "            discriminator_loss = -(\n",
    "                torch.mean(discriminator_real) -\n",
    "                torch.mean(discriminator_fake)\n",
    "            )\n",
    "\n",
    "            discriminator_loss.backward() # backpropagate the loss through the discriminator network\n",
    "            discriminator_solver.step() # update the discriminator network parameters using the optimizer\n",
    "\n",
    "            # Reset the gradients\n",
    "            generator.zero_grad()\n",
    "            discriminator.zero_grad()\n",
    "\n",
    "        # Sample and score fake data\n",
    "        noise = torch.randn(batch_size, noise_dim)\n",
    "        fake_sample = generator(noise)\n",
    "        discriminator_fake = discriminator(fake_sample)\n",
    "\n",
    "        # Calculate generator loss\n",
    "        # Generator wants discriminator to assign a high score to fake data\n",
    "        ### CHANGED: new loss function ###\n",
    "        generator_loss = -torch.mean(discriminator_fake)\n",
    "\n",
    "        generator_loss.backward() # backpropagate the loss through the generator network\n",
    "        generator_solver.step() # update the generator network parameters using the optimizer\n",
    "\n",
    "        # Reset the gradients\n",
    "        generator.zero_grad()\n",
    "        discriminator.zero_grad()\n",
    "\n",
    "        # Show training losses and sample crosstabs after each epoch\n",
    "        if int(iteration % epoch_length) == 0:\n",
    "            epoch = int(iteration / epoch_length)\n",
    "            print('Epoch {}\\n'\n",
    "                  'Discriminator loss: {}; '\n",
    "                  'Generator loss: {}'.format(epoch,\n",
    "                                              discriminator_loss.data.numpy(),\n",
    "                                              generator_loss.data.numpy()))\n",
    "            noise = torch.randn(len(data), noise_dim) # noise for fake sample\n",
    "            fake_sample = generator(noise)\n",
    "            # convert back from one-hot encoding\n",
    "            fake_df = pd.DataFrame(enc.inverse_transform(fake_sample.detach()))\n",
    "            fake_df.columns = df.columns\n",
    "            show_heatmap(fake_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train(data, generator, discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentially-private WGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generator = torch.nn.Sequential(\n",
    "    torch.nn.Linear(noise_dim, hidden_dim),\n",
    "    torch.nn.ReLU(),\n",
    "    MultiCategoryGumbelSoftmax(hidden_dim, output_dims)\n",
    ")\n",
    "\n",
    "\n",
    "discriminator = torch.nn.Sequential(\n",
    "    torch.nn.Linear(data_dim, hidden_dim),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(hidden_dim, 1)\n",
    ")\n",
    "\n",
    "def train(data, generator, discriminator,\n",
    "          epochs=20, n_discriminator=5, batch_size=128,\n",
    "          learning_rate=1e-3, sigma=0.01, weight_clip=0.1): ### CHANGED: add sigma and weight_clip parameters ###\n",
    "    \"\"\"Train the model\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : torch.Tensor\n",
    "        Data for training\n",
    "    epochs : int\n",
    "        Number of iterations over the full data set for training\n",
    "    n_discriminator : int\n",
    "        Number of discriminator training iterations\n",
    "    batch_size : int\n",
    "        Number of training examples per inner iteration\n",
    "    learning_rate : float\n",
    "        Learning rate for training\n",
    "    sigma : float\n",
    "        Amount of noise to add (for differential privacy)\n",
    "    weight_clip : float\n",
    "        Maximum range of weights (for differential privacy)\n",
    "    \"\"\"\n",
    "    # these solvers are optimizers for learning the parameters for our networks\n",
    "    # we have one for each network (the generator and the discriminator)\n",
    "    # RMSprop is one of many optimizers, and the choice is not terribly important here\n",
    "    # The learning rate influences how large of a step the optimizer takes when it updates the parameters\n",
    "    generator_solver = torch.optim.RMSprop(\n",
    "        generator.parameters(), lr=learning_rate\n",
    "    )\n",
    "    discriminator_solver = torch.optim.RMSprop(\n",
    "        discriminator.parameters(), lr=learning_rate\n",
    "    )\n",
    "\n",
    "    ### CHANGED: add hooks to introduce noise to gradient for differential privacy ###\n",
    "    for parameter in discriminator.parameters():\n",
    "        parameter.register_hook(\n",
    "            lambda grad: grad + sigma * torch.randn(parameter.shape)\n",
    "        )\n",
    "\n",
    "    # There is a batch for each discriminator training iteration,\n",
    "    # so each epoch is epoch_length iterations, and the total number of\n",
    "    # iterations is the number of epochs times the length of each epoch.\n",
    "    epoch_length = len(data) / (n_discriminator * batch_size)\n",
    "    n_iters = int(epochs * epoch_length)\n",
    "    for iteration in range(n_iters):\n",
    "        for _ in range(n_discriminator):\n",
    "            # Sample real data\n",
    "            rand_perm = torch.randperm(data.size(0))\n",
    "            real_sample = data[rand_perm[:batch_size]]\n",
    "\n",
    "            # Sample fake data\n",
    "            noise = torch.randn(batch_size, noise_dim)\n",
    "            fake_sample = generator(noise)\n",
    "\n",
    "            # Have the discriminator score the data\n",
    "            discriminator_real = discriminator(real_sample)\n",
    "            discriminator_fake = discriminator(fake_sample)\n",
    "\n",
    "            # Calculate discriminator loss\n",
    "            # Discriminator wants to assign a high score to real data\n",
    "            # and a low score to fake data          \n",
    "            discriminator_loss = -(\n",
    "               torch.mean(discriminator_real) -\n",
    "               torch.mean(discriminator_fake)\n",
    "            )\n",
    "\n",
    "            discriminator_loss.backward()\n",
    "            discriminator_solver.step()\n",
    "\n",
    "            ### CHANGED: add weight clipping for privacy guarantee ###\n",
    "            for param in discriminator.parameters():\n",
    "               param.data.clamp_(-weight_clip, weight_clip)\n",
    "\n",
    "            # Reset gradient\n",
    "            generator.zero_grad()\n",
    "            discriminator.zero_grad()\n",
    "\n",
    "        # Sample and score fake data\n",
    "        noise = torch.randn(batch_size, noise_dim)\n",
    "        fake_sample = generator(noise)\n",
    "        discriminator_fake = discriminator(fake_sample)\n",
    "\n",
    "        # Calculate generator loss\n",
    "        # Generator wants discriminator to assign a high score to fake data\n",
    "        generator_loss = -torch.mean(discriminator_fake)\n",
    "\n",
    "        generator_loss.backward()\n",
    "        generator_solver.step()\n",
    "\n",
    "        # Reset gradient\n",
    "        generator.zero_grad()\n",
    "        discriminator.zero_grad()\n",
    "\n",
    "        # Show training losses and sample crosstabs after each epoch\n",
    "        if int(iteration % epoch_length) == 0:\n",
    "            epoch = int(iteration / epoch_length)\n",
    "            print('Epoch {}\\n'\n",
    "                  'Discriminator loss: {}; '\n",
    "                  'Generator loss: {}'.format(epoch,\n",
    "                                              discriminator_loss.data.numpy(),\n",
    "                                              generator_loss.data.numpy()))\n",
    "            noise = torch.randn(len(data), noise_dim) # noise for fake sample\n",
    "            fake_sample = generator(noise)\n",
    "            # convert back from one-hot encoding\n",
    "            fake_df = pd.DataFrame(enc.inverse_transform(fake_sample.detach()))\n",
    "            fake_df.columns = df.columns\n",
    "            show_heatmap(fake_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train(data, generator, discriminator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
